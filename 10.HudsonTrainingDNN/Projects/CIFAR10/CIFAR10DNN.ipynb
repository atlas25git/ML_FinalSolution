{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10DNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk7whPYapqjX"
      },
      "source": [
        "## 8. Deep Learning on CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWmlAl6PpqjX"
      },
      "source": [
        "### a.\n",
        "*Exercise: Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vi4v7R7pqjY"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100,\n",
        "                                 activation=\"elu\",\n",
        "                                 kernel_initializer=\"he_normal\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb3UI_yHpqjY"
      },
      "source": [
        "### b.\n",
        "*Exercise: Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with `keras.datasets.cifar10.load_data()`. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ63FRYApqjY"
      },
      "source": [
        "Let's add the output layer to the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI3G1KVDpqjY"
      },
      "source": [
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8-rDsVXpqjY"
      },
      "source": [
        "Let's use a Nadam optimizer with a learning rate of 5e-5. I tried learning rates 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 and 1e-2, and I compared their learning curves for 10 epochs each (using the TensorBoard callback, below). The learning rates 3e-5 and 1e-4 were pretty good, so I tried 5e-5, which turned out to be slightly better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BFHcQwWpqjY"
      },
      "source": [
        "optimizer = keras.optimizers.Nadam(lr=5e-5)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sze5IcIpqjZ"
      },
      "source": [
        "Let's load the CIFAR10 dataset. We also want to use early stopping, so we need a validation set. Let's use the first 5,000 images of the original training set as the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC7g7nPfpqjZ"
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "X_train = X_train_full[5000:]\n",
        "y_train = y_train_full[5000:]\n",
        "X_valid = X_train_full[:5000]\n",
        "y_valid = y_train_full[:5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qEbF74jpqjZ"
      },
      "source": [
        "Now we can create the callbacks we need and train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGLJ5vpGpqjZ"
      },
      "source": [
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
        "run_index = 1 # increment every time you train the model\n",
        "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M8yRJaapqja",
        "outputId": "06e1db27-e5c7-4320-caca-883f78c0d219"
      },
      "source": [
        "%tensorboard --logdir=./my_cifar10_logs --port=6006"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with 255).\n",
              "Contents of stderr:\n",
              "E0213 19:12:55.493896 4621630912 program.py:311] TensorBoard could not bind to port 6006, it was already in use\n",
              "ERROR: TensorBoard could not bind to port 6006, it was already in use"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37rVXc1Ypqja",
        "outputId": "899d0c8a-71ac-4c40-b1fd-31090d419a1d"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=100,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1407/1407 [==============================] - 9s 5ms/step - loss: 9.4191 - accuracy: 0.1388 - val_loss: 2.2328 - val_accuracy: 0.2040\n",
            "Epoch 2/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 2.1097 - accuracy: 0.2317 - val_loss: 2.0485 - val_accuracy: 0.2402\n",
            "Epoch 3/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.9667 - accuracy: 0.2844 - val_loss: 1.9681 - val_accuracy: 0.2964\n",
            "Epoch 4/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.8740 - accuracy: 0.3149 - val_loss: 1.9178 - val_accuracy: 0.3254\n",
            "Epoch 5/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.8064 - accuracy: 0.3423 - val_loss: 1.8256 - val_accuracy: 0.3384\n",
            "Epoch 6/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.7525 - accuracy: 0.3595 - val_loss: 1.7430 - val_accuracy: 0.3692\n",
            "Epoch 7/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.7116 - accuracy: 0.3819 - val_loss: 1.7199 - val_accuracy: 0.3824\n",
            "Epoch 8/100\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 1.6782 - accuracy: 0.3935 - val_loss: 1.6746 - val_accuracy: 0.3972\n",
            "Epoch 9/100\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 1.6517 - accuracy: 0.4025 - val_loss: 1.6622 - val_accuracy: 0.4004\n",
            "Epoch 10/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.6140 - accuracy: 0.4194 - val_loss: 1.7065 - val_accuracy: 0.3840\n",
            "Epoch 11/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5884 - accuracy: 0.4301 - val_loss: 1.6736 - val_accuracy: 0.3914\n",
            "Epoch 12/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5640 - accuracy: 0.4378 - val_loss: 1.6220 - val_accuracy: 0.4224\n",
            "Epoch 13/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5437 - accuracy: 0.4448 - val_loss: 1.6332 - val_accuracy: 0.4144\n",
            "Epoch 14/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5214 - accuracy: 0.4555 - val_loss: 1.5785 - val_accuracy: 0.4326\n",
            "Epoch 15/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5117 - accuracy: 0.4564 - val_loss: 1.6267 - val_accuracy: 0.4164\n",
            "Epoch 16/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4972 - accuracy: 0.4622 - val_loss: 1.5846 - val_accuracy: 0.4316\n",
            "Epoch 17/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4888 - accuracy: 0.4661 - val_loss: 1.5549 - val_accuracy: 0.4420\n",
            "Epoch 18/100\n",
            "<<24 more lines>>\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 1.3362 - accuracy: 0.5212 - val_loss: 1.6025 - val_accuracy: 0.4500\n",
            "Epoch 31/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3360 - accuracy: 0.5207 - val_loss: 1.5175 - val_accuracy: 0.4602\n",
            "Epoch 32/100\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 1.3031 - accuracy: 0.5302 - val_loss: 1.5397 - val_accuracy: 0.4572\n",
            "Epoch 33/100\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 1.3082 - accuracy: 0.5308 - val_loss: 1.4997 - val_accuracy: 0.4776\n",
            "Epoch 34/100\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 1.2882 - accuracy: 0.5338 - val_loss: 1.5482 - val_accuracy: 0.4620\n",
            "Epoch 35/100\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 1.2889 - accuracy: 0.5355 - val_loss: 1.5474 - val_accuracy: 0.4604\n",
            "Epoch 36/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2761 - accuracy: 0.5410 - val_loss: 1.5434 - val_accuracy: 0.4658\n",
            "Epoch 37/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2658 - accuracy: 0.5481 - val_loss: 1.5502 - val_accuracy: 0.4706\n",
            "Epoch 38/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2554 - accuracy: 0.5489 - val_loss: 1.5527 - val_accuracy: 0.4624\n",
            "Epoch 39/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2504 - accuracy: 0.5471 - val_loss: 1.5482 - val_accuracy: 0.4602\n",
            "Epoch 40/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2516 - accuracy: 0.5545 - val_loss: 1.5881 - val_accuracy: 0.4574\n",
            "Epoch 41/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2401 - accuracy: 0.5566 - val_loss: 1.5403 - val_accuracy: 0.4670\n",
            "Epoch 42/100\n",
            "1407/1407 [==============================] - 8s 5ms/step - loss: 1.2305 - accuracy: 0.5570 - val_loss: 1.5343 - val_accuracy: 0.4790\n",
            "Epoch 43/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2228 - accuracy: 0.5615 - val_loss: 1.5344 - val_accuracy: 0.4708\n",
            "Epoch 44/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2161 - accuracy: 0.5619 - val_loss: 1.5782 - val_accuracy: 0.4526\n",
            "Epoch 45/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2124 - accuracy: 0.5641 - val_loss: 1.5182 - val_accuracy: 0.4794\n",
            "Epoch 46/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1870 - accuracy: 0.5766 - val_loss: 1.5435 - val_accuracy: 0.4650\n",
            "Epoch 47/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1925 - accuracy: 0.5701 - val_loss: 1.5532 - val_accuracy: 0.4686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb1d8ef8790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydgDhsPIpqja",
        "outputId": "89ed14ee-d478-4dde-c965-5ff279ac6be7"
      },
      "source": [
        "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
        "model.evaluate(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 0s 1ms/step - loss: 1.4960 - accuracy: 0.4762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4960416555404663, 0.47620001435279846]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEFAmiMvpqja"
      },
      "source": [
        "The model with the lowest validation loss gets about 47.6% accuracy on the validation set. It took 27 epochs to reach the lowest validation loss, with roughly 8 seconds per epoch on my laptop (without a GPU). Let's see if we can improve performance using Batch Normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LbSo9oMpqjb"
      },
      "source": [
        "### c.\n",
        "*Exercise: Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNtPfjpVpqjb"
      },
      "source": [
        "The code below is very similar to the code above, with a few changes:\n",
        "\n",
        "* I added a BN layer after every Dense layer (before the activation function), except for the output layer. I also added a BN layer before the first hidden layer.\n",
        "* I changed the learning rate to 5e-4. I experimented with 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3 and 3e-3, and I chose the one with the best validation performance after 20 epochs.\n",
        "* I renamed the run directories to run_bn_* and the model file name to my_cifar10_bn_model.h5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DzxPsJGpqjb",
        "outputId": "86d2066e-c7b6-41cb-859b-1182106d87d7"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Activation(\"elu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
        "run_index = 1 # increment every time you train the model\n",
        "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
        "\n",
        "model.fit(X_train, y_train, epochs=100,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
        "model.evaluate(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1407/1407 [==============================] - 19s 9ms/step - loss: 1.9765 - accuracy: 0.2968 - val_loss: 1.6602 - val_accuracy: 0.4042\n",
            "Epoch 2/100\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6787 - accuracy: 0.4056 - val_loss: 1.5887 - val_accuracy: 0.4304\n",
            "Epoch 3/100\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6097 - accuracy: 0.4274 - val_loss: 1.5781 - val_accuracy: 0.4326\n",
            "Epoch 4/100\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5574 - accuracy: 0.4486 - val_loss: 1.5064 - val_accuracy: 0.4676\n",
            "Epoch 5/100\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5075 - accuracy: 0.4642 - val_loss: 1.4412 - val_accuracy: 0.4844\n",
            "Epoch 6/100\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4664 - accuracy: 0.4787 - val_loss: 1.4179 - val_accuracy: 0.4984\n",
            "Epoch 7/100\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4334 - accuracy: 0.4932 - val_loss: 1.4277 - val_accuracy: 0.4906\n",
            "Epoch 8/100\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 1.4054 - accuracy: 0.5038 - val_loss: 1.3843 - val_accuracy: 0.5130\n",
            "Epoch 9/100\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 1.3816 - accuracy: 0.5106 - val_loss: 1.3691 - val_accuracy: 0.5108\n",
            "Epoch 10/100\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 1.3547 - accuracy: 0.5206 - val_loss: 1.3552 - val_accuracy: 0.5226\n",
            "Epoch 11/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3244 - accuracy: 0.5371 - val_loss: 1.3678 - val_accuracy: 0.5142\n",
            "Epoch 12/100\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 1.3078 - accuracy: 0.5393 - val_loss: 1.3844 - val_accuracy: 0.5080\n",
            "Epoch 13/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2889 - accuracy: 0.5431 - val_loss: 1.3566 - val_accuracy: 0.5164\n",
            "Epoch 14/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2607 - accuracy: 0.5559 - val_loss: 1.3626 - val_accuracy: 0.5248\n",
            "Epoch 15/100\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 1.2580 - accuracy: 0.5587 - val_loss: 1.3616 - val_accuracy: 0.5276\n",
            "Epoch 16/100\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 1.2441 - accuracy: 0.5586 - val_loss: 1.3350 - val_accuracy: 0.5286\n",
            "Epoch 17/100\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 1.2241 - accuracy: 0.5676 - val_loss: 1.3370 - val_accuracy: 0.5408\n",
            "Epoch 18/100\n",
            "<<29 more lines>>\n",
            "Epoch 33/100\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 1.0336 - accuracy: 0.6369 - val_loss: 1.3682 - val_accuracy: 0.5450\n",
            "Epoch 34/100\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0228 - accuracy: 0.6388 - val_loss: 1.3348 - val_accuracy: 0.5458\n",
            "Epoch 35/100\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 1.0205 - accuracy: 0.6407 - val_loss: 1.3490 - val_accuracy: 0.5440\n",
            "Epoch 36/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 1.0008 - accuracy: 0.6489 - val_loss: 1.3568 - val_accuracy: 0.5408\n",
            "Epoch 37/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 0.9785 - accuracy: 0.6543 - val_loss: 1.3628 - val_accuracy: 0.5396\n",
            "Epoch 38/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 0.9832 - accuracy: 0.6592 - val_loss: 1.3617 - val_accuracy: 0.5482\n",
            "Epoch 39/100\n",
            "1407/1407 [==============================] - 12s 8ms/step - loss: 0.9707 - accuracy: 0.6581 - val_loss: 1.3767 - val_accuracy: 0.5446\n",
            "Epoch 40/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 0.9590 - accuracy: 0.6651 - val_loss: 1.4200 - val_accuracy: 0.5314\n",
            "Epoch 41/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 0.9548 - accuracy: 0.6668 - val_loss: 1.3692 - val_accuracy: 0.5450\n",
            "Epoch 42/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 0.9480 - accuracy: 0.6667 - val_loss: 1.3841 - val_accuracy: 0.5310\n",
            "Epoch 43/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 0.9411 - accuracy: 0.6716 - val_loss: 1.4036 - val_accuracy: 0.5382\n",
            "Epoch 44/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 0.9383 - accuracy: 0.6708 - val_loss: 1.4114 - val_accuracy: 0.5236\n",
            "Epoch 45/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 0.9258 - accuracy: 0.6769 - val_loss: 1.4224 - val_accuracy: 0.5324\n",
            "Epoch 46/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 0.9072 - accuracy: 0.6836 - val_loss: 1.3875 - val_accuracy: 0.5442\n",
            "Epoch 47/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 0.8996 - accuracy: 0.6850 - val_loss: 1.4449 - val_accuracy: 0.5280\n",
            "Epoch 48/100\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9050 - accuracy: 0.6835 - val_loss: 1.4167 - val_accuracy: 0.5338\n",
            "Epoch 49/100\n",
            "1407/1407 [==============================] - 12s 9ms/step - loss: 0.8934 - accuracy: 0.6880 - val_loss: 1.4260 - val_accuracy: 0.5294\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 1.3344 - accuracy: 0.5398\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3343921899795532, 0.5397999882698059]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4RjV_Djpqjb"
      },
      "source": [
        "* *Is the model converging faster than before?* Much faster! The previous model took 27 epochs to reach the lowest validation loss, while the new model achieved that same loss in just 5 epochs and continued to make progress until the 16th epoch. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.\n",
        "* *Does BN produce a better model?* Yes! The final model is also much better, with 54.0% accuracy instead of 47.6%. It's still not a very good model, but at least it's much better than before (a Convolutional Neural Network would do much better, but that's a different topic, see chapter 14).\n",
        "* *How does BN affect training speed?* Although the model converged much faster, each epoch took about 12s instead of 8s, because of the extra computations required by the BN layers. But overall the training time (wall time) was shortened significantly!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2bxQBICpqjc"
      },
      "source": [
        "### d.\n",
        "*Exercise: Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "r3fV0lBUpqjc",
        "outputId": "39193ac2-c3c3-49a0-8380-941f26722e1a"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100,\n",
        "                                 kernel_initializer=\"lecun_normal\",\n",
        "                                 activation=\"selu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = keras.optimizers.Nadam(lr=7e-4)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
        "run_index = 1 # increment every time you train the model\n",
        "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
        "\n",
        "X_means = X_train.mean(axis=0)\n",
        "X_stds = X_train.std(axis=0)\n",
        "X_train_scaled = (X_train - X_means) / X_stds\n",
        "X_valid_scaled = (X_valid - X_means) / X_stds\n",
        "X_test_scaled = (X_test - X_means) / X_stds\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=100,\n",
        "          validation_data=(X_valid_scaled, y_valid),\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
        "model.evaluate(X_valid_scaled, y_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1407/1407 [==============================] - 10s 5ms/step - loss: 2.0622 - accuracy: 0.2631 - val_loss: 1.7878 - val_accuracy: 0.3552\n",
            "Epoch 2/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.7328 - accuracy: 0.3830 - val_loss: 1.7028 - val_accuracy: 0.3828\n",
            "Epoch 3/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.6342 - accuracy: 0.4279 - val_loss: 1.6692 - val_accuracy: 0.4022\n",
            "Epoch 4/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5524 - accuracy: 0.4538 - val_loss: 1.6350 - val_accuracy: 0.4300\n",
            "Epoch 5/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4979 - accuracy: 0.4756 - val_loss: 1.5773 - val_accuracy: 0.4356\n",
            "Epoch 6/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4428 - accuracy: 0.4902 - val_loss: 1.5529 - val_accuracy: 0.4630\n",
            "Epoch 7/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3966 - accuracy: 0.5126 - val_loss: 1.5290 - val_accuracy: 0.4682\n",
            "Epoch 8/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3549 - accuracy: 0.5232 - val_loss: 1.4633 - val_accuracy: 0.4792\n",
            "Epoch 9/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3162 - accuracy: 0.5444 - val_loss: 1.4787 - val_accuracy: 0.4776\n",
            "Epoch 10/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2825 - accuracy: 0.5534 - val_loss: 1.4794 - val_accuracy: 0.4934\n",
            "Epoch 11/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2529 - accuracy: 0.5682 - val_loss: 1.5529 - val_accuracy: 0.4982\n",
            "Epoch 12/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2256 - accuracy: 0.5784 - val_loss: 1.4942 - val_accuracy: 0.4902\n",
            "Epoch 13/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2049 - accuracy: 0.5823 - val_loss: 1.4868 - val_accuracy: 0.5024\n",
            "Epoch 14/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1627 - accuracy: 0.6012 - val_loss: 1.4839 - val_accuracy: 0.5082\n",
            "Epoch 15/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1543 - accuracy: 0.6034 - val_loss: 1.5097 - val_accuracy: 0.4968\n",
            "Epoch 16/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1200 - accuracy: 0.6135 - val_loss: 1.5001 - val_accuracy: 0.5120\n",
            "Epoch 17/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1028 - accuracy: 0.6199 - val_loss: 1.4856 - val_accuracy: 0.5056\n",
            "Epoch 18/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0863 - accuracy: 0.6265 - val_loss: 1.5116 - val_accuracy: 0.4966\n",
            "Epoch 19/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0715 - accuracy: 0.6345 - val_loss: 1.5787 - val_accuracy: 0.5070\n",
            "Epoch 20/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0342 - accuracy: 0.6453 - val_loss: 1.4987 - val_accuracy: 0.5144\n",
            "Epoch 21/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0169 - accuracy: 0.6531 - val_loss: 1.6292 - val_accuracy: 0.4462\n",
            "Epoch 22/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1346 - accuracy: 0.6074 - val_loss: 1.5280 - val_accuracy: 0.5136\n",
            "Epoch 23/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9820 - accuracy: 0.6678 - val_loss: 1.5392 - val_accuracy: 0.5040\n",
            "Epoch 24/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9701 - accuracy: 0.6679 - val_loss: 1.5505 - val_accuracy: 0.5170\n",
            "Epoch 25/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3604 - accuracy: 0.6753 - val_loss: 1.5468 - val_accuracy: 0.4992\n",
            "Epoch 26/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0177 - accuracy: 0.6510 - val_loss: 1.5474 - val_accuracy: 0.5020\n",
            "Epoch 27/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9425 - accuracy: 0.6798 - val_loss: 1.5545 - val_accuracy: 0.5076\n",
            "Epoch 28/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9005 - accuracy: 0.6902 - val_loss: 1.5659 - val_accuracy: 0.5138\n",
            "157/157 [==============================] - 0s 1ms/step - loss: 1.4633 - accuracy: 0.4792\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4633383750915527, 0.47920000553131104]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWu_PYjUpqjc",
        "outputId": "eea33c8f-3b1d-4d47-a625-c45791c2e38d"
      },
      "source": [
        "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
        "model.evaluate(X_valid_scaled, y_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 0s 1ms/step - loss: 1.4633 - accuracy: 0.4792\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4633383750915527, 0.47920000553131104]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZqH7fH0pqjc"
      },
      "source": [
        "We get 47.9% accuracy, which is not much better than the original model (47.6%), and not as good as the model using batch normalization (54.0%). However, convergence was almost as fast as with the BN model, plus each epoch took only 7 seconds. So it's by far the fastest model to train so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQwpLdjWpqjd"
      },
      "source": [
        "### e.\n",
        "*Exercise: Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtutQv_fpqjd",
        "outputId": "453c383a-419b-4151-e039-eb38129a7931"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100,\n",
        "                                 kernel_initializer=\"lecun_normal\",\n",
        "                                 activation=\"selu\"))\n",
        "\n",
        "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
        "run_index = 1 # increment every time you train the model\n",
        "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
        "\n",
        "X_means = X_train.mean(axis=0)\n",
        "X_stds = X_train.std(axis=0)\n",
        "X_train_scaled = (X_train - X_means) / X_stds\n",
        "X_valid_scaled = (X_valid - X_means) / X_stds\n",
        "X_test_scaled = (X_test - X_means) / X_stds\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=100,\n",
        "          validation_data=(X_valid_scaled, y_valid),\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
        "model.evaluate(X_valid_scaled, y_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1407/1407 [==============================] - 9s 5ms/step - loss: 2.0583 - accuracy: 0.2742 - val_loss: 1.7429 - val_accuracy: 0.3858\n",
            "Epoch 2/100\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 1.6852 - accuracy: 0.4008 - val_loss: 1.7055 - val_accuracy: 0.3792\n",
            "Epoch 3/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5963 - accuracy: 0.4413 - val_loss: 1.7401 - val_accuracy: 0.4072\n",
            "Epoch 4/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5231 - accuracy: 0.4634 - val_loss: 1.5728 - val_accuracy: 0.4584\n",
            "Epoch 5/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4619 - accuracy: 0.4887 - val_loss: 1.5448 - val_accuracy: 0.4702\n",
            "Epoch 6/100\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 1.4074 - accuracy: 0.5061 - val_loss: 1.5678 - val_accuracy: 0.4664\n",
            "Epoch 7/100\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 1.3718 - accuracy: 0.5222 - val_loss: 1.5764 - val_accuracy: 0.4824\n",
            "Epoch 8/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3220 - accuracy: 0.5387 - val_loss: 1.4805 - val_accuracy: 0.4890\n",
            "Epoch 9/100\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 1.2908 - accuracy: 0.5487 - val_loss: 1.5521 - val_accuracy: 0.4638\n",
            "Epoch 10/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2537 - accuracy: 0.5607 - val_loss: 1.5281 - val_accuracy: 0.4924\n",
            "Epoch 11/100\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 1.2215 - accuracy: 0.5782 - val_loss: 1.5147 - val_accuracy: 0.5046\n",
            "Epoch 12/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1910 - accuracy: 0.5831 - val_loss: 1.5248 - val_accuracy: 0.5002\n",
            "Epoch 13/100\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 1.1659 - accuracy: 0.5982 - val_loss: 1.5620 - val_accuracy: 0.5066\n",
            "Epoch 14/100\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 1.1282 - accuracy: 0.6120 - val_loss: 1.5440 - val_accuracy: 0.5180\n",
            "Epoch 15/100\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 1.1127 - accuracy: 0.6133 - val_loss: 1.5782 - val_accuracy: 0.5146\n",
            "Epoch 16/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0917 - accuracy: 0.6266 - val_loss: 1.6182 - val_accuracy: 0.5182\n",
            "Epoch 17/100\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 1.0620 - accuracy: 0.6331 - val_loss: 1.6285 - val_accuracy: 0.5126\n",
            "Epoch 18/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0433 - accuracy: 0.6413 - val_loss: 1.6299 - val_accuracy: 0.5158\n",
            "Epoch 19/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0087 - accuracy: 0.6549 - val_loss: 1.7172 - val_accuracy: 0.5062\n",
            "Epoch 20/100\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.9950 - accuracy: 0.6571 - val_loss: 1.6524 - val_accuracy: 0.5098\n",
            "Epoch 21/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9848 - accuracy: 0.6652 - val_loss: 1.7686 - val_accuracy: 0.5038\n",
            "Epoch 22/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9597 - accuracy: 0.6744 - val_loss: 1.6177 - val_accuracy: 0.5084\n",
            "Epoch 23/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9399 - accuracy: 0.6790 - val_loss: 1.7095 - val_accuracy: 0.5082\n",
            "Epoch 24/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9148 - accuracy: 0.6884 - val_loss: 1.7160 - val_accuracy: 0.5150\n",
            "Epoch 25/100\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.9023 - accuracy: 0.6949 - val_loss: 1.7017 - val_accuracy: 0.5152\n",
            "Epoch 26/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.8732 - accuracy: 0.7031 - val_loss: 1.7274 - val_accuracy: 0.5088\n",
            "Epoch 27/100\n",
            "1407/1407 [==============================] - 6s 5ms/step - loss: 0.8542 - accuracy: 0.7091 - val_loss: 1.7648 - val_accuracy: 0.5166\n",
            "Epoch 28/100\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.8499 - accuracy: 0.7118 - val_loss: 1.7973 - val_accuracy: 0.5000\n",
            "157/157 [==============================] - 0s 1ms/step - loss: 1.4805 - accuracy: 0.4890\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4804893732070923, 0.48899999260902405]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEAboNUqpqje"
      },
      "source": [
        "The model reaches 48.9% accuracy on the validation set. That's very slightly better than without dropout (47.6%). With an extensive hyperparameter search, it might be possible to do better (I tried dropout rates of 5%, 10%, 20% and 40%, and learning rates 1e-4, 3e-4, 5e-4, and 1e-3), but probably not much better in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s32I1iqBpqje"
      },
      "source": [
        "Let's use MC Dropout now. We will need the `MCAlphaDropout` class we used earlier, so let's just copy it here for convenience:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jFWr8g6pqje"
      },
      "source": [
        "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
        "    def call(self, inputs):\n",
        "        return super().call(inputs, training=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S116M4EFpqje"
      },
      "source": [
        "Now let's create a new model, identical to the one we just trained (with the same weights), but with `MCAlphaDropout` dropout layers instead of `AlphaDropout` layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiGiz_Ftpqje"
      },
      "source": [
        "mc_model = keras.models.Sequential([\n",
        "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
        "    for layer in model.layers\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgLyholvpqje"
      },
      "source": [
        "Then let's add a couple utility functions. The first will run the model many times (10 by default) and it will return the mean predicted class probabilities. The second will use these mean probabilities to predict the most likely class for each instance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg-SFQOepqjf"
      },
      "source": [
        "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
        "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
        "    return np.mean(Y_probas, axis=0)\n",
        "\n",
        "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
        "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
        "    return np.argmax(Y_probas, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6qC5pP4pqjf"
      },
      "source": [
        "Now let's make predictions for all the instances in the validation set, and compute the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9TPa2iypqjf",
        "outputId": "7aede9e5-9574-4a87-b12c-ce685d96f57e"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
        "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4892"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmzmeWNapqjf"
      },
      "source": [
        "We get no accuracy improvement in this case (we're still at 48.9% accuracy).\n",
        "\n",
        "So the best model we got in this exercise is the Batch Normalization model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzuayU9Epqjf"
      },
      "source": [
        "### f.\n",
        "*Exercise: Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II9Wh3pvpqjg"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100,\n",
        "                                 kernel_initializer=\"lecun_normal\",\n",
        "                                 activation=\"selu\"))\n",
        "\n",
        "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = keras.optimizers.SGD(lr=1e-3)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxW83lJXpqjg",
        "outputId": "5bfbf2a1-bd75-42ab-c61c-eb591de454e8"
      },
      "source": [
        "batch_size = 128\n",
        "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
        "plot_lr_vs_loss(rates, losses)\n",
        "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "352/352 [==============================] - 2s 6ms/step - loss: nan - accuracy: 0.1255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9.999999747378752e-06,\n",
              " 9.999868392944336,\n",
              " 2.6167492866516113,\n",
              " 3.9354368618556435)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMvytzpNpqjg"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100,\n",
        "                                 kernel_initializer=\"lecun_normal\",\n",
        "                                 activation=\"selu\"))\n",
        "\n",
        "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = keras.optimizers.SGD(lr=1e-2)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7zm9RbFpqjh",
        "outputId": "0b3597d8-d90a-4280-e8da-2b95804763aa"
      },
      "source": [
        "n_epochs = 15\n",
        "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
        "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
        "                    validation_data=(X_valid_scaled, y_valid),\n",
        "                    callbacks=[onecycle])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "352/352 [==============================] - 3s 6ms/step - loss: 2.2298 - accuracy: 0.2349 - val_loss: 1.7841 - val_accuracy: 0.3834\n",
            "Epoch 2/15\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 1.7928 - accuracy: 0.3689 - val_loss: 1.6806 - val_accuracy: 0.4086\n",
            "Epoch 3/15\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 1.6475 - accuracy: 0.4190 - val_loss: 1.6378 - val_accuracy: 0.4350\n",
            "Epoch 4/15\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 1.5428 - accuracy: 0.4543 - val_loss: 1.6266 - val_accuracy: 0.4390\n",
            "Epoch 5/15\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 1.4865 - accuracy: 0.4769 - val_loss: 1.6158 - val_accuracy: 0.4384\n",
            "Epoch 6/15\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 1.4339 - accuracy: 0.4866 - val_loss: 1.5850 - val_accuracy: 0.4412\n",
            "Epoch 7/15\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 1.4042 - accuracy: 0.5056 - val_loss: 1.6146 - val_accuracy: 0.4384\n",
            "Epoch 8/15\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 1.3437 - accuracy: 0.5229 - val_loss: 1.5299 - val_accuracy: 0.4846\n",
            "Epoch 9/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 1.2721 - accuracy: 0.5459 - val_loss: 1.5145 - val_accuracy: 0.4874\n",
            "Epoch 10/15\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 1.1942 - accuracy: 0.5698 - val_loss: 1.4958 - val_accuracy: 0.5040\n",
            "Epoch 11/15\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 1.1211 - accuracy: 0.6033 - val_loss: 1.5406 - val_accuracy: 0.4984\n",
            "Epoch 12/15\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 1.0673 - accuracy: 0.6161 - val_loss: 1.5284 - val_accuracy: 0.5144\n",
            "Epoch 13/15\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.9927 - accuracy: 0.6435 - val_loss: 1.5449 - val_accuracy: 0.5140\n",
            "Epoch 14/15\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.9205 - accuracy: 0.6703 - val_loss: 1.5652 - val_accuracy: 0.5224\n",
            "Epoch 15/15\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.8936 - accuracy: 0.6801 - val_loss: 1.5912 - val_accuracy: 0.5198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JhYlIYupqjh"
      },
      "source": [
        "One cycle allowed us to train the model in just 15 epochs, each taking only 2 seconds (thanks to the larger batch size). This is several times faster than the fastest model we trained so far. Moreover, we improved the model's performance (from 47.6% to 52.0%). The batch normalized model reaches a slightly better performance (54%), but it's much slower to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4_DfLvLpqjh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}